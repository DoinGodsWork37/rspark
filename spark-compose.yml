version: '2'
services:
  postgres:
    build: ./postgres
    image: rsprk-postgres
    ports:
      - "5432:5432"
    volumes:
      - rspark_pgdata:/opt/pg-data
  hive:
    build: ./hive
    image: rsprk-hive
    ports:
      - "10000:10000"
    depends_on:
      - hadoop
  hadoop:
    build: ./hadoop
    image: rsprk-hadoop
    volumes:
      - rspark_hadoopdata:/opt/hadoop
    ports:
      - "9000:9000" # Hadoop
      - "50070:50070" # HadoopUI Access
  rstudio:
    build: ./rstudio
    restart: always
    image: rsprk-rstudio
    environment:
      - PASSWORD=rstudiojh
    ports:
      - "8787:8787" # RstudioUI
    links:
      - hadoop
      - postgres
      - master
    volumes:
      - rspark_rstudio:/home/rstudio
    depends_on: 
      - hadoop
      - hive
  master:
    build: ./spark/sprk_master
    image: rsprk-sprkmaster
    volumes:
      - rspark_sparkdata:/opt/hdfs
    ports:
      - "6066:6066"
      - "7070:7070"
      - "7077:7077"
      - "8080:8080"
#      - "50070:50070"
  worker:
#    build: ./spark/sprk_worker
    image: rsprk-sprkmaster
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    ports:
      - "8081:8081"
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      #SPARK_WORKER_PORT: 8881
      #SPARK_WORKER_WEBUI_PORT: 8081
      #SPARK_PUBLIC_DNS: localhost
    links:
      - master
    depends_on:
      - master

volumes:
  rspark_hadoopdata:
  rspark_rstudio:
  rspark_pgdata:
  rspark_sparkdata:
